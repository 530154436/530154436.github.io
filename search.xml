<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[第2章 感知机]]></title>
    <url>%2F2019%2F01%2F27%2F%E7%AC%AC2%E7%AB%A0%20%E6%84%9F%E7%9F%A5%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[2.1 模型假设输入空间是 $x \subseteq R^n$，输出空间是$y={+1, -1}$，x和y分属这两个空间，那么由输入空间到输出空间的如下函数：$$f(x) = sign(w \cdot x +b)$$称为感知机。其中，w和b称为感知机模型参数，$w \subseteq R^n$叫做权值或权值向量，$b \in R$叫做偏置(bias)，$w \cdot x$表示向量w和x的内积。sign是一个函数：$$ sign(x) = \begin{cases} 1 &amp; x \geq 0 \\ -1 &amp; x \leq 0 \end{cases}$$感知机是定义在特征空间中的所有线性分类模型或线性分类器，，属于判别模型，即函数集合$\{f|f(x)=w \cdot x+b\} $，其几何解释是，线性方程$$w \cdot x + b = 0$$将特征空间划分为正负两个部分：这个平面（2维时退化为直线）称为分离超平面(separating hyperplane，$n&gt;3$)。 2.2 学习策略2.2.1 数据集的线性可分性给定数据集$$T = \{ (x_1, y_1),(x_2, y_2), \cdots (x_N, y_N) \}$$其中 $x_i \in X = R^n，y_i \in Y = {+1, -1},，i = 1,2,\cdots,N$，如果存在某个超平面S$$w \cdot x + b = 0$$能够完全正确地将正负实例点全部分割开来，则称T线性可分，否则称T线性不可分。 2.2.2 学习策略(损失函数)假定数据集线性可分，我们希望找到一个合理的损失函数。(1) 采用误分类点的总数但是这样的损失函数不是参数w，b的连续可导函数，不可导自然不能把握函数的变化，也就不易优化（不知道什么时候该终止训练，或终止的时机不是最优的）。(2) 选择所有误分类点到超平面S的总距离点$x_0$到平面S的距离 ：$$\frac {w \cdot x_0 +b} {||w||}$$其中，$||w||$是w的$L_2$范数，$||w|| = \sqrt { w_1^2 + w_2^2 + \cdots + w_N^2}$ 。类比点到平面的距离，此处的点到超平面S的距离的几何意义就是上述距离在多维空间的推广。$$ d(x_i, y_i) = \frac {|ax_i + by_i + c|} { \sqrt { a^2 + b^2} }$$若点$i$被误分类一定有$$-y_i ( w \cdot x_i + b) &gt; 0$$成立，所以我们去掉了绝对值符号，得到误分类点到超平面S的距离公式：$$ - \frac {y_i (w \cdot x_i +b) } {||w||}$$假设所有误分类点构成集合M，那么所有误分类点到超平面S的总距离为$$ - \frac 1 {||w||} \sum_{x_i \in M}{y_i (w \cdot x_i +b) } $$分母作用不大，反正一定是正的，不考虑分母，就得到了感知机学习的损失函数：$$ L(w,b) = - \frac 1 {||w||} \sum_{x_i \in M}{y_i (w \cdot x_i +b) } $$ 2.3 学习算法2.3.1 原始形式感知机学习算法是对以下最优化问题的算法：$$\text{min} L(w,b) = -\sum_{x_i \in M}{y_i (w \cdot x_i +b) }$$感知机学习算法是误分类驱动的，先随机选取一个超平面(误分类点)，然后用梯度下降法不断极小化上述损失函数。损失函数的梯度：$$\nabla_w L(w,b) = -\sum_{x_i \in M}{y_ix_i} \\\nabla_b L(w,b) = -\sum_{x_i \in M}{y_i}$$随机选一个误分类点$x_i$，对参数w，b进行更新：$$w \leftarrow w + \eta y_i x_i \\b \leftarrow b + \eta y_i$$上式$\eta (0 &lt; \eta \leq 1)$是学习率。损失函数的参数加上梯度上升的反方向，于是就梯度下降了。所以，上述迭代可以使损失函数不断减小，直到为0。于是得到了原始形式的感知机学习算法： $$\begin{array}{cl}算法1 &amp; 感知机学习算法的原始形式 \\\hline输入 &amp; T = \{ (x_1, y_1),(x_2, y_2), \cdots (x_N, y_N) \} \\输出 &amp; w，b；感知机模型: f(x) = sign(w \cdot x + b) \\(1) &amp; 选取初始值 w_0, b_0 \\(2) &amp; 在训练集中选取数据 (x_i, x_i) \\(3) &amp; 若 y_i(w \cdot x_i +b) \leq 0 \\ &amp; w \leftarrow w + \eta y_i x_i ；b \leftarrow b + \eta y_i \\(4) &amp; 转至（2），直至训练集中没有误分类点\end{array}$$ 算法的收敛性Novikoff定理 $\to$ 表明经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。2.3.2 对偶形式 基本思想(对偶) 将w和b表示为实例 $x_i$ 和标记 $y_i$ 的线性组合形式，通过求解系数得到w和b。具体说来，如果对误分类点 $x_i$ 逐步修改wb修改了n次，则w，b关于$ (x_i, y_i)$ 的增量分别为 $\alpha_i y_i x_i$ 和 $\alpha_i y_i $，这里$\alpha_i = n_i \eta $，则最终求解到的参数分别表示为：$$w = \sum_{i=1} ^N {\alpha_i y_i x_i}\\b = \sum_{i=1} ^N {\alpha_i y_i}$$ 于是有算法2.2：$$\begin{array}{cl}算法1 &amp; 感知机学习算法的原始形式 \\\hline输入 &amp; T = \{ (x_1, y_1),(x_2, y_2), \cdots (x_N, y_N) \} \\输出 &amp; \alpha，b；感知机模型: f(x) = sign(\sum_{j=1} ^N {\alpha_j y_j x_j} \cdot x + b) \\&amp; 其中，\alpha = (\alpha_1, \alpha_2, \cdots , \alpha_N)^T \\(1) &amp; 选取初始值 \alpha \leftarrow 0, b \leftarrow 0 \\(2) &amp; 在训练集中选取数据 (x_i, x_i) \\(3) &amp; 若 y_i(\sum_{j=1} ^N {\alpha_j y_j x_j} \cdot x_i + b) \leq 0 \\&amp; \alpha_i \leftarrow \alpha + \eta ；b \leftarrow b + \eta y_i \\(4) &amp; 转至（2），直至训练集中没有误分类点\end{array}$$由于训练实例仅以内积的形式出现，为方便，可预先将训练集中实例间的内积计算出来并以矩阵形式存储，这就是所谓的Gram矩阵。$$G = [x_i \cdot x_j]_{N \times N}$$2.4 算法实现 原始形式的感知机算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445weights = []b = 0print_pattern = 'Iteration &#123;&#125;, 误分类点: x&#123;&#125;, w=&#123;&#125;, b=&#123;&#125;'def update(item, learning_rate = 1): ''' 更新权值、偏置 ''' global weights, b for i in range(len(item)): weights[i] += learning_rate * item[1] * item[0][i] b += learning_rate * item[1]def cal(item): ''' 计算 yi*(w*xi + b) &lt;= 0 ''' res = 0 for i in range(len(item[0])): res += item[0][i] * weights[i] res += b res *= item[1] return resdef check(iteration, learning_rate = 1): # 标识是否存在误分类点 flag = False for xi in range(len(data_set)): if cal(data_set[xi]) &lt;= 0: flag = True update(data_set[xi], learning_rate=learning_rate) print(print_pattern.format(iteration, xi + 1, weights, b)) # 误分类点不存在，迭代结束 return flagdef train(data_set, max_iteration=1000, learning_rate=1): ''' 感知机学习算法的原始形式 ''' for i in range(len(data_set)): weights.append(0) iteration = 0 print(print_pattern.format(iteration, 1, weights, b)) while True: if not check(iteration) or iteration&gt;max_iteration: break iteration += 1if __name__ == '__main__': # 训练集 data_set = [[(3, 3), 1], [(4, 3), 1], [(1, 1), -1]] train(data_set) 对偶形式的感知机算法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import numpy as npprint_pattern = 'Iteration &#123;&#125;, 误分类点: x&#123;&#125;, alphas=&#123;&#125;, b=&#123;&#125;'class Perceptron(): def __init__(self, max_iteration=1000, learning_rate=1): # 参数向量 self.alphas = None # Gram 矩阵 self.gram = None # 偏置 self.b = 0 # 迭代次数 self.iteration = 0 # 最大迭代次数 self.max_iteration = max_iteration # 学习率 self.learning_rate = learning_rate def __init(self, data_set): ''' 计算 Gram 矩阵、初始化 alphas :return: ''' N = len(data_set) self.alphas = np.zeros(N, dtype=np.float) self.gram = np.zeros(shape=(N, N), dtype=int) for i in range(len(data_set)): for j in range(len(data_set)): # 求内积 -&gt; np.dot 点乘或矩阵乘法 self.gram[i][j] = np.dot(data_set[i][0], data_set[j][0]) return self.gram def __cal(self, i, y): ''' 计算距离 ''' res = np.dot(self.alphas * y, self.gram[i]) + self.b res *= y[i] return res def __update(self, i, y): ''' 更新权值、偏置 ''' self.alphas[i] += self.learning_rate self.b += self.learning_rate * y[i] def __check(self, y): ''' 标识是否存在误分类点 ''' flag = False for i in range(len(y)): if self.__cal(i, y) &lt;= 0: flag = True self.__update(i, y) self.iteration += 1 print(print_pattern.format(self.iteration, i+1, self.alphas, self.b)) # 误分类点不存在，迭代结束 return flag def train(self, data_set): ''' 感知机学习算法的对偶形式 ''' self.__init(data_set) if type(data_set) is not np.ndarray: data_set = np.array(data_set) # 分类标签 y = data_set[:, 1] while True: # 标识是否存在误分类点或是否大于最大迭代次数 if not self.__check(y) or self.iteration &gt; self.max_iteration: breakif __name__ == '__main__': perceptron = Perceptron() # 训练集 data_set = [[(3, 3), 1], [(4, 3), 1], [(1, 1), -1]] perceptron.train(data_set) 2.5 引用 http://www.hankcs.com/ml/the-perceptron.htmlhttps://www.cnblogs.com/naonaoling/p/5690219.htmlhttp://www.cnblogs.com/OldPanda/archive/2013/04/12/3017100.html]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第1章 统计学习方法概论]]></title>
    <url>%2F2019%2F01%2F27%2F%E7%AC%AC1%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[1.1 统计学习方法1.1.1 监督学习 (supervised learning)利用训练数据集(输入\输出数据对）学习一个模型，再用模型对测试样本集进行预测。主要用于分类、标注、回归分析。 联合概率分布 监督问题的形式化 分类、标注、回归 对比| 监督学习 | 输入变量| 输出变量| 应用 ||:—:| :—:|:—:| :—:|| 分类 | 离散或连续| 离散| 文本分类、客户类型分类、垃圾邮件过滤等 || 标注 | 观测序列| 状态(标记)序列| 词性标注、信息抽取等 || 回归 |连续 | 连续| 股价预测问题等 | 分类问题评价标准 TP —— 将正类预测为正类 FN —— 将正类预测为负类 FP —— 将负类预测为正类 TP —— 将负类预测为负类 精确率：$P=\frac {TP} {TP+FP}$ $\to$ 预测正确的正类/预测为正类的总数 召回率：$R=\frac {TP} {TP+FN} $ $\to$ 预测正确的正类/总的正类 F1值：$ \frac 2 {F_1} = \frac 1 P + \frac 1 R $ $\to$ 精确率和召回率的调和均值，两个率都高时，F1也会高 1.1.2 非监督学习 (unsupervised learning)用于学习的数据集只有输入（未标记的样本），学习的任务是对于数据进行分析，找到输出。主要用于聚类。 1.1.3 半监督学习监督学习和非监督学习的结合，它主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题，主要用于半监督分类、半监督回归、半监督聚类、半监督降维。 1.1.4 强化学习学习者在学习过程中不断与环境交互，会从环境中得到一定的奖赏，根据奖赏再不断的学习，直到达到一个更优的策略。 1.1.5 实现统计学习方法的步骤(1) 得到一个有限的训练数据集合；(2) 确定包含所有可能的模型的假设空间，即学习模型的集合；(3) 确定模型选则的准则，即策略；(4) 实现求解最优模型的算法，即算法；(5) 选择最优的算法；(6) 利用最优模型对新数据进行预测或分析。 1.2 统计学习三要素1.2.1 模型即所要学习的条件概率分布或决策函数 1.2.1.1 假设空间包含所有可能的条件概率分布或决策函数，可以定义为决策函数的集合或条件概率分布族，即$$ F = \{ f | Y = f(X)\} 或 F = \{ P | P( Y|X ) \}$$ 1.2.1.2 参数空间包含决策函数或条件概率分布模型中涉及的所有参数向量$$ F = \{ f | Y = f_\theta (X)，\theta \in R^n \} 或 F = \{ P | P_\theta ( Y|X )，\theta \in R^n \}$$ 1.2.2 策略1.2.2.1 损失函数与风险函数 损失函数（loss function）或代价函数（cost function）对于给定的输入x，由模型f(X)给出相应的输出，但是预测的输出f(x)与真实值Y可能存在不一致，用一个损失函数或者代价函数来度量预测错误的程度。损失函数L(Y,f(X))是预测值f(X)和真实值Y的非负实值函数。损失函数值越小，模型就越好。常见的损失函数： 0-1损失函数$$ L(Y, f(X)) = \begin{cases} 1 &amp; Y \neq f(x) \\ 0 &amp;Y=f(X) \end{cases}$$ 平方损失函数$$ L(Y,f(X)) = (Y-f(X))^2 $$ 绝对损失函数$$ L(Y,f(X)) = |Y-f(X)| $$ 对数损失函数( logarithmic loss function )或对数似然损失函数$$ L(Y,f(X)) = -logP(Y|X) $$ 风险函数( risk function ) 或 期望损失 ( expected loss )模型$f(x)$关于联合分布$P(X, Y) $的平均意义下的损失:$$ R_{exp} = E_P[ L( Y, f(X)) ] = \int_{X \cdot Y} L( Y, f(X)) P(X, Y) dxdy $$ 经验风险 (empirical risk)模型$f(X)$关于训练数据集的平均损失，根据大数定律，当样本容量N趋于无穷时，经验风险趋于期望风险，所以可以用经验风险估计期望风险。$$ R_{emp} = \frac 1 N \sum_{i=1}^N L(y_i,f(x_i))$$其中，$ T=\{ (x_1, y_1),(x_2, y_2) \cdots (x_N, y_N) \} $ 为训练集 1.2.2.2 经验风险最小化和结构风险最小化 经验风险最小化（Empirical Risk Minimization，ERM）ERM的策略认为经验风险最小的模型是最优的模型 $\to$ 极大似然估计（某些条件下），即$$ min \{ R_{emp} \}$$ 当样本容量足够大时，经验风险最小化能保证有很好的学习效果 当样本容量很小时，经验风险最小化学习的效果未必很好，甚至会产生“过拟合”问题 结构风险最小化（structural risk minimization，SRM）为了防止过拟合而提出的策略，结构风险在经验风险上加上表示模型复杂度的正则化项（regularization）或罚项（penalty term）。SRM策略认为结构风险最小的模型就是最优模型 $\to$ 贝叶斯估计中的最大后验概率估计。$$ min \{ R_{emp} + \lambda J(f) \}$$其中，$J(f)$ 为模型的复杂度，$f \in F$ 1.2.2 算法$\to$ 经验风险或结构风险最优化问题 1.3 模型评估与模型选择1.3.1 训练误差与测试误差 训练误差模型关于训练数据集的平均损失（经验风险）$$ R_{emp} (\hat f) = \frac 1 N \sum_{i=1}^N L(y_i,\hat f(x_i))$$ 测试误差模型关于测试数据集的平均损失（经验风险）$$ e_{test}= \frac 1 {N^{‘}} \sum_{i=1}^{N^{‘}} L(y_i,\hat f(x_i))$$ 误差率 (error rate)$$ e_{test}= \frac 1 {N^{‘}} \sum_{i=1}^{N^{‘}} I(y_i \neq \hat f(x_i))$$其中，$I$ 为指示函数( indicator function )，$ I = \begin{cases} 1 &amp; y \neq \hat f(x) \\ 0 &amp; y= \hat f(x) \end{cases}$ 准确率 (accracy rate)$$ r_{test}= \frac 1 {N^{‘}} \sum_{i=1}^{N^{‘}} I(y_i = \hat f(x_i))$$ 1.3.2 过拟合 过拟合 (over-fitting)学习时选择的模型所包含的参数过多（复杂度过高），以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。1.3.3 模型选择 1.3.3.1 正则化 ( regularization )正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。作用是选择经验风险与模型复杂度同时较小的模型。正则化项的不同形式: $L_2$范数 (平方损失) $$L(w) = \frac 1 N \sum_{i=1}^N ( f(x_i; w) - y_i ) + \frac \lambda 2 || w ||_2^2$$ 其中，w 为参数向量，$|| w ||_2 = \sqrt { \sum_{i=1}^N w_i^2 }$ $L_1$范数 (绝对值损失) $$L(w) = \frac 1 N \sum_{i=1}^N ( f(x_i; w) - y_i ) + \lambda || w ||_1$$1.3.3.2 交叉验证如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切成三部分，分别为训练集、验证集（validation set）和测试集。训练集用来训练模型，验证集用于模型的选择，测试集用于最终对方法的评估。但是由于在许多实际应用中数据是不充分的，为了选择好的模型，可以采用交叉验证方法。 基本思想重复的使用数据，把给定的数据进行切分，将切分的数据集组合为训练集和测试集，在此基础上反复地进行训练、测试以及模型选择 方法 简单交叉验证将已给数据随机分为两部分，分别用作训练集和测试集；然后用训练集在各种条件下，训练模型，从而得到不同的模型；最后用测试集评价模型。 S折交叉验证 (S-fold cross validation)首先将已给数据随机分为S个互不相交、大小相同的子集；然后利用S-1个子集的数据训练模型，剩余1个子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。 留一交叉验证S折交叉验证的特殊形式是S=N，其中N是给定数据集的容量 1.4 泛化能力指由该方法学习到的模型对未知数据的预测能力。 泛化误差 (generalization error)现实中，可以通过测试误差来评价学习方法的泛化能力（测试数据集的经验风险），但是由于测试数据集有限，所以从理论上进行分析：用学习到的模型对未知数据预测的误差即为泛化误差（测试数据集的期望风险）$$ R_{exp} (\hat f) = E_P[ L( Y, \hat f(X)) ] = \int_{x \cdot y} L( Y, \hat f(X)) P(X, Y) dxdy $$ 泛化误差上界可以理解为泛化误差的可能最大值，等于经验风险+一个函数（参数是样本容量和假设空间容量），即$$ R(f) \leq \hat R(f) + \epsilon(d,N, \delta) $$ 泛化误差上界是样本容量($N$)的单调递减函数，当样本容量增加时，泛化上界趋于0 泛化误差上界也是假设空间容量($d$)的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大 1.5 生成模型和判别模型监督学习方法可以分为生成方法和判别方法，所学到的模型分别称为生成模型和判别模型。 生成模型 (generative model)由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：$$ P(Y|X)= \frac {P(X,Y)} {P(X)} $$之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。 典型的生成模型朴素贝叶斯法和隐马尔可夫模型 优点可以还原联合概率分布P(X,Y)；学习收敛速度更快；存在隐变量时，仍可以用生成方法学习 判别模型 (discriminative model)由数据直接学习决策函数f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。判别方法关心的是对给定的输入X，应该预测什么样的输出Y。 典型的判别模型K近邻、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法、条件随机场等 优点准确率更高；简化学习问题 1.6 引用 https://www.cnblogs.com/naonaoling/p/5689830.html]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
